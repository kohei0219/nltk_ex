{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "このレッスンでは、PythonのライブラリであるNLTK(Natural Language Toolkit)を使用してTweetの感情分析を行います。その過程で文章の前処理、形態素解析などを学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1.Dataset<br>\n",
    "2.Embedding and Morphological Analysis<br>\n",
    "3.Sentiment Analysis<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "NLTKのcorpusには、Twitterのサンプルがあり、positive, negative, 感情がないツイートの3種類のデータがあります。<br>\n",
    "まずは、このデータを取得します。<br>\n",
    "まだインストールしていない場合はコメントアウトを外してインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('punkt') # import word tokenization\n",
    "# nltk.download('averaged_perceptron_tagger')  # import pos tagger\n",
    "# nltk.download('wordnet') # import Lemmatisation\n",
    "# nltk.download('stopwords') # import stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweetのデータを読み込みました。まず、Tweetの内容を少し確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive tweets length: 5000\n",
      "=======Example========\n",
      "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!']\n",
      "======================\n",
      "negative tweets length: 5000\n",
      "=======Example========\n",
      "['hopeless for tmr :(', \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\", '@Hegelbon That heart sliding into the waste basket. :(']\n",
      "======================\n",
      "all tweets length: 20000\n"
     ]
    }
   ],
   "source": [
    "print('positive tweets length:', len(positive_tweets))\n",
    "print('=======Example========')\n",
    "print(positive_tweets[0:3])\n",
    "print('======================')\n",
    "print('negative tweets length:', len(negative_tweets))\n",
    "print('=======Example========')\n",
    "print(negative_tweets[0:3])\n",
    "print('======================')\n",
    "print('all tweets length:', len(all_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のデータをみると分かるように、__顔文字によってtweetが分類されています。__<br>\n",
    ":) -> Positive<br>\n",
    ":( -> Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "まず初めに、文章を単語ずつに区切ります。そのために、用いるのが先ほどnltk.download('punkt')でダウンロードした分かち書き(word tokenization)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'FollowFriday', '@', 'France_Inte', '@', 'PKuchly57', '@', 'Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "text = positive_tweets[0]\n",
    "split = nltk.word_tokenize(text)\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、正規化を行います。例えば,goという単語にはgo, goes, went, goingなどのようにたくさんの種類の形があります。<br>\n",
    "これらを同じ形式(go)に統一していきます。<br>\n",
    "その際に、品詞の情報が必要となるので、品詞のタグ付けを行っていきます。\n",
    "### Morphological Analysis<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', '#'), ('FollowFriday', 'NNP'), ('@', 'NNP'), ('France_Inte', 'NNP'), ('@', 'NNP'), ('PKuchly57', 'NNP'), ('@', 'NNP'), ('Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':', ':'), (')', ')')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tagged = pos_tag(split)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規化していきます。今回は、名詞（posがNNから始まるもの）と動詞（posがVBから始まるもの）を正規化します。<br>\n",
    "正規化にはWordNetLemmatizerを使用し、引数に単語とその品詞を渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'FollowFriday', '@', 'France_Inte', '@', 'PKuchly57', '@', 'Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentence = []\n",
    "for word, tag in tagged:\n",
    "    if tag.startswith('NN'):\n",
    "        pos = 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "        pos = 'v'\n",
    "    else:\n",
    "        pos = 'a'\n",
    "    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このTweetの内容だと、__engagedがengageに、membersがmember__に変換さてれいることが確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノイズ除去\n",
    "\"a\"や\"the\"のように特定の意味を持たない単語を取り除いていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "after: ['despiteofficial', 'we', 'had', 'listen', 'last', 'night', 'as', 'you', 'bleed', 'is', 'amazing', 'track', 'when', 'are', 'you', 'in', 'scotland']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "stop_words = [\"a\", \"an\"]\n",
    "res = []\n",
    "print('before:', positive_tweets[2])\n",
    "\n",
    "for word in nltk.word_tokenize(positive_tweets[2]):\n",
    "    # remove url\n",
    "    token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                          '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n",
    "    token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "    if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "        res.append(token.lower())\n",
    "print('after:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex1\n",
    "上述のコードを使用して、テキストを前処理する関数を作成しましょう。<br>\n",
    "## 手順\n",
    "1.文章を単語ごとにわける<br>\n",
    "2.ストップワードでノイズを除去する(stop_wordsを使う)<br>\n",
    "3.単語を正規化する<br>\n",
    "\n",
    "引数：positive_tweets<br>\n",
    "返り値：前処理されたセンテンスの配列\n",
    "sentence = [];\n",
    "return List(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalEmbedding(sentences):\n",
    "    res = []\n",
    "    # write your code!!\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "after: ['followfriday', 'france_inte', 'pkuchly57', 'milipol_paris', 'top', 'engage', 'member', 'community', 'week']\n",
      "before: hopeless for tmr :(\n",
      "after: ['hopeless', 'tmr']\n"
     ]
    }
   ],
   "source": [
    "embed_positive_sentences = NormalEmbedding(positive_tweets)\n",
    "print(\"before:\", positive_tweets[0])\n",
    "print(\"after:\", embed_positive_sentences[0])\n",
    "\n",
    "embed_negative_sentences = NormalEmbedding(negative_tweets)\n",
    "print(\"before:\", negative_tweets[0])\n",
    "print(\"after:\", embed_negative_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Well Done!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "positive, negativeなtweetを学習させて、tweetを分類できるようにします。<br>\n",
    "まずは、単語の辞書を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(tokens_list):\n",
    "    for tweet_tokens in tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens_for_model = get_tweets_for_model(embed_positive_sentences)\n",
    "negative_tokens_for_model = get_tweets_for_model(embed_negative_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainデータとtestデータを作成する。\n",
    "今回は、validationデータは省きます。<br>\n",
    "trainデータに全体の70%のデータを、testデータに全体の30%のデータ与えます。<br>\n",
    "作成後に、これらのデータをシャッフルします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length: 7000\n",
      "[({'mhmmdiqbale': True, 'happy': True, 'birthday': True, 'iqbal': True, 'wish': True, 'best': True}, 'Positive'), ({'theyingster': True, 'cristianavai3': True, 'afcgramachroi': True, 'goonerlover69': True, 'iwaveback': True, 'lol': True, \"'s\": True, 'tiring': True, 'good': True, 'look': True}, 'Positive'), ({'gazetteboro': True, 'often_partisan': True, 'pip': True, 'post': True, 'think': True}, 'Negative')]\n",
      "========================\n",
      "test data length 3000\n",
      "[({'deltadaily': True, 'deltagoodrem': True, 'love': True, 'new': True, 'song': True, 'delta': True, 'rock': True}, 'Positive'), ({'carva': True}, 'Negative'), ({'jairaabells': True, 'wo': True, \"n't\": True, 'around': True, 'tom': True}, 'Negative')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "balance=[0.7,0.3]\n",
    "train_num = int(len(positive_tweets) * balance[0])\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                    for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                    for tweet_dict in negative_tokens_for_model]\n",
    "train_dataset = positive_dataset[:train_num] + negative_dataset[:train_num]\n",
    "test_dataset = positive_dataset[train_num:] + negative_dataset[train_num:]\n",
    "\n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(test_dataset)\n",
    "print(\"train data length:\", len(train_dataset))\n",
    "print(train_dataset[:3])\n",
    "print(\"========================\")\n",
    "print(\"test data length\", len(test_dataset))\n",
    "print(test_dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの構築とテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類器を用いてtrainデータセットで学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7463333333333333\n",
      "Most Informative Features\n",
      "                       p = True           Positi : Negati =     58.3 : 1.0\n",
      "                  arrive = True           Positi : Negati =     33.0 : 1.0\n",
      "                    glad = True           Positi : Negati =     23.7 : 1.0\n",
      "                      ff = True           Positi : Negati =     23.0 : 1.0\n",
      "                    sick = True           Negati : Positi =     19.7 : 1.0\n",
      "                     sad = True           Negati : Positi =     19.4 : 1.0\n",
      "               community = True           Positi : Negati =     17.0 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.3 : 1.0\n",
      "                    miss = True           Negati : Positi =     12.7 : 1.0\n",
      "            justinbieber = True           Negati : Positi =     12.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_dataset))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex2\n",
    "顔文字が含まれるように前処理した場合のデータでトレーニングしてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__今回は、twitter_sample用のtokenizerが定義されているので、それを使用したいと思います。__<br>\n",
    "以下のように分かち書きをすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'], ['@Lamb2ja', 'Hey', 'James', '!', 'How', 'odd', ':/', 'Please', 'call', 'our', 'Contact', 'Centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'Many', 'thanks', '!']]\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顔文字が1文字として認識できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n",
      "before: hopeless for tmr :(\n",
      "['hopeless', 'tmr', ':(']\n"
     ]
    }
   ],
   "source": [
    "# まずはEmbedding用の関数を定義しましょう\n",
    "def Embedding(sentences):\n",
    "    res = []\n",
    "    # write your code!!\n",
    "    \n",
    "    return res\n",
    "\n",
    "embed_positive_sentences = Embedding....... # write your code\n",
    "print(\"before:\", positive_tweets[0])\n",
    "print(embed_positive_sentences[0])\n",
    "\n",
    "embed_negative_sentences = Embedding....... # write your code\n",
    "print(\"before:\", negative_tweets[0])\n",
    "print(embed_negative_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# トレーニングデータとテストデータを作成しましょう\n",
    "# write your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.992\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2071.0 : 1.0\n",
      "                      :) = True           Positi : Negati =   1005.4 : 1.0\n",
      "                follower = True           Positi : Negati =     39.7 : 1.0\n",
      "                followed = True           Negati : Positi =     34.3 : 1.0\n",
      "                  arrive = True           Positi : Negati =     33.0 : 1.0\n",
      "                    glad = True           Positi : Negati =     23.7 : 1.0\n",
      "                     x15 = True           Negati : Positi =     23.7 : 1.0\n",
      "                     sad = True           Negati : Positi =     19.9 : 1.0\n",
      "                    sick = True           Negati : Positi =     19.7 : 1.0\n",
      "               community = True           Positi : Negati =     16.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# trainingとtestを行い、結果を比較しましょう\n",
    "# write your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Well Done!!!!!!!!!!!!!!!!__<br><br>\n",
    "前処理によって精度が変わることが確認できました。<br>\n",
    "もし、顔文字で判定したいのであればEx2のような前処理が有効的で、顔文字をデータを分けるために使用して単語で判定したい場合はEx1の方法が有効的であると言えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra excercise\n",
    "自由にtweetを作成して推測してみましょう。<br>\n",
    "その際に、二種類の方法でトレーニングした場合の未知の文章の推測を比べてみましょう。<br>\n",
    "NormalEmbeddingとEmbaddingを切り替えたりしても良いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT @OwenJones84: @Shiny02 @AndyYoung90 @Kelvinbhoy @meljomur http://t.co/9y3W44E76L I want a Labour government backed up by the SNP.']\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, 20000)\n",
    "custom_tweet = [all_tweets[index]]\n",
    "print(custom_tweet)\n",
    "custom_tokens = NormalEmbedding(custom_tweet)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please write your own sentences here']\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = [\"Please write your own sentences here\"]\n",
    "print(custom_tweet)\n",
    "custom_tokens = NormalEmbedding(custom_tweet)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
